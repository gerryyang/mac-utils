---
layout: post
title:  "Consistent Hashing in Action"
date:   2024-05-27 12:30:00 +0800
categories: åŸºç¡€ç†è®º
---

* Do not remove this line (it will not be displayed)
{:toc}

# èƒŒæ™¯ä»‹ç»

> **Consistent Hashing** is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table. It powers many high-traffic dynamic websites and web applications.

In recent years, with the advent(å‡ºç°) of concepts such as cloud computing(äº‘è®¡ç®—) and big data(å¤§æ•°æ®), distributed systems(åˆ†å¸ƒå¼ç³»ç»Ÿ) have gained popularity and relevance.

One such type of system, distributed caches(åˆ†å¸ƒå¼ç¼“å­˜) that power many high-traffic dynamic websites and web applications, typically consist of a particular case of distributed hashing. These take advantage of an algorithm known as **consistent hashing**.

# åŸºæœ¬ä»‹ç»


In computer science, **consistent hashing** is a special kind of [hashing technique](https://en.wikipedia.org/wiki/Hash_function) such that when a hash table is resized, only `n/m` keys need to be remapped on average where `n` is the number of **keys** and `m` is the number of **slots**. **In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation**.

Consistent hashing evenly distributes cache keys across shards, even if some of the shards crash or become unavailable [3].

> Consistent hashing æ˜¯ä¸€ç§ç‰¹æ®Šçš„å“ˆå¸ŒæŠ€æœ¯ï¼Œä¸»è¦ç”¨äºè§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ•°æ®åˆ†é…é—®é¢˜ã€‚å…¶ä¸»è¦ä¼˜ç‚¹æ˜¯èƒ½å¤Ÿåœ¨èŠ‚ç‚¹æ•°é‡å˜åŒ–æ—¶ï¼Œæœ€å°åŒ–éœ€è¦é‡æ–°åˆ†é…çš„æ•°æ®æ•°é‡ã€‚è¿™å¯¹äºç¼“å­˜ç³»ç»Ÿç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºåœ¨èŠ‚ç‚¹å¢åŠ æˆ–å‡å°‘æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½å‡å°‘å› ä¸ºé‡æ–°å“ˆå¸Œå¸¦æ¥çš„ç¼“å­˜å¤±æ•ˆã€‚
>
> åœ¨ä¼ ç»Ÿçš„å“ˆå¸Œè¡¨ä¸­ï¼Œå¦‚æœå“ˆå¸Œè¡¨çš„å¤§å°æ”¹å˜ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºæ·»åŠ æˆ–åˆ é™¤äº†èŠ‚ç‚¹ï¼‰ï¼Œå‡ ä¹æ‰€æœ‰çš„é”®éƒ½éœ€è¦é‡æ–°å“ˆå¸Œåˆ°æ–°çš„ä½ç½®ã€‚è¿™å¯¹äºç¼“å­˜ç³»ç»Ÿæ¥è¯´æ˜¯ä¸å¯æ¥å—çš„ï¼Œå› ä¸ºè¿™æ„å‘³ç€å¤§é‡çš„ç¼“å­˜å¤±æ•ˆå’Œé‡å»ºã€‚
>
> è€Œåœ¨ä¸€è‡´æ€§å“ˆå¸Œä¸­ï¼Œå½“æ·»åŠ æˆ–åˆ é™¤èŠ‚ç‚¹æ—¶ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†çš„é”®ä¼šè¢«æ˜ å°„åˆ°æ–°çš„èŠ‚ç‚¹ï¼Œå¤§éƒ¨åˆ†çš„é”®ä»ç„¶ä¼šè¢«æ˜ å°„åˆ°åŸæ¥çš„èŠ‚ç‚¹ã€‚è¿™å°±æ„å‘³ç€ï¼Œå³ä½¿æœ‰èŠ‚ç‚¹å´©æºƒæˆ–è€…å˜å¾—ä¸å¯ç”¨ï¼Œä¹Ÿåªä¼šå½±å“åˆ°ä¸€å°éƒ¨åˆ†çš„é”®ã€‚
>
> æ­¤å¤–ï¼Œä¸€è‡´æ€§å“ˆå¸Œè¿˜èƒ½ä¿è¯æ•°æ®åœ¨èŠ‚ç‚¹ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒã€‚è¿™æ˜¯é€šè¿‡å°†æ¯ä¸ªèŠ‚ç‚¹å’Œæ¯ä¸ªé”®éƒ½æ˜ å°„åˆ°ä¸€ä¸ªç¯å½¢çš„å“ˆå¸Œç©ºé—´æ¥å®ç°çš„ã€‚æ¯ä¸ªé”®éƒ½è¢«åˆ†é…åˆ°é¡ºæ—¶é’ˆæ–¹å‘ä¸Šçš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆä¹Ÿå°±æ˜¯æœ€è¿‘çš„èŠ‚ç‚¹ï¼‰ã€‚å› æ­¤ï¼Œå³ä½¿èŠ‚ç‚¹çš„æ•°é‡å‘ç”Ÿå˜åŒ–ï¼Œé”®ä¹Ÿèƒ½è¢«å‡åŒ€åœ°åˆ†é…åˆ°å„ä¸ªèŠ‚ç‚¹ã€‚
>
> æ€»çš„æ¥è¯´ï¼Œä¸€è‡´æ€§å“ˆå¸Œèƒ½å¤Ÿåœ¨èŠ‚ç‚¹æ•°é‡å˜åŒ–æ—¶ï¼Œæœ€å°åŒ–éœ€è¦é‡æ–°åˆ†é…çš„æ•°æ®æ•°é‡ï¼Œå¹¶ä¸”ä¿è¯æ•°æ®åœ¨èŠ‚ç‚¹ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒã€‚è¿™ä½¿å¾—å®ƒéå¸¸é€‚åˆç”¨äºåˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿã€‚


ä¸€è‡´æ€§å“ˆå¸Œçš„ç‰¹ç‚¹ï¼š

1. **å¹³è¡¡æ€§**ã€‚ä¸åŒ Key é€šè¿‡ç®—æ³•æ˜ å°„åï¼Œå¯ä»¥æ¯”è¾ƒå‡è¡¡åœ°åˆ†å¸ƒåˆ°æ‰€æœ‰çš„èŠ‚ç‚¹ä¸Š
2. **å•è°ƒæ€§**ã€‚å½“æœ‰æ–°çš„èŠ‚ç‚¹ä¸Šçº¿åï¼Œç³»ç»Ÿä¸­åŸæœ‰çš„ Key è¦ä¹ˆè¿˜æ˜¯æ˜ å°„åˆ°åŸæ¥çš„èŠ‚ç‚¹ä¸Šï¼Œè¦ä¹ˆæ˜ å°„åˆ°æ–°åŠ å…¥çš„èŠ‚ç‚¹ä¸Šï¼Œä¸ä¼šå‡ºç°ä»ä¸€ä¸ªè€èŠ‚ç‚¹é‡æ–°æ˜ å°„åˆ°å¦ä¸€ä¸ªè€èŠ‚ç‚¹
3. **ç¨³å®šæ€§**ã€‚å½“æœåŠ¡å‘ç”Ÿæ‰©ç¼©å®¹çš„æ—¶å€™ï¼Œå‘ç”Ÿè¿ç§»çš„æ•°æ®é‡å°½å¯èƒ½çš„å°‘



# What Is Hashing?

A hash function is a function that maps one piece of dataâ€”typically describing some kind of object, often of arbitrary sizeâ€”to another piece of data, typically an integer, known as hash code, or simply hash.

For instance, some hash function designed to hash strings, with an output range of `0 .. 100`, may map the string `Hello` to, say, the number `57`, `Hasta la vista, baby` to the number `33`, and any other possible string to some number within that range. Since there are way more possible inputs than outputs, any given number will have many different strings mapped to it, a phenomenon(ç°è±¡) known as **collision(ç¢°æ’)**. Good hash functions should somehow â€œchop and mixâ€ (hence the term) the input data in such a way that the outputs for different input values are spread as evenly as possible over the output range.

Hash functions have many uses and for each one, different properties may be desired. There is a type of hash function known as **cryptographic hash functions**, which must meet a restrictive set of properties and are used for security purposesâ€”including applications such as password protection, integrity checking and fingerprinting of messages, and data corruption detection, among others, but those are outside the scope of this article.

**Non-cryptographic hash functions** have several uses as well, the most common being their use in hash tables, which is the one that concerns us and which weâ€™ll explore in more detail.

# Introducing Hash Tables (Hash Maps)

Imagine we needed to keep a list of all the members of some club while being able to search for any specific member. We could handle it by keeping the list in an array (or linked list) and, to perform a search, iterate the elements until we find the desired one (we might be searching based on their name, for instance). In the worst case, that would mean checking all members (if the one weâ€™re searching for is last, or not present at all), or half of them on average. In complexity theory terms, the search would then have complexity `O(n)`, and it would be reasonably fast for a small list, but it would get slower and slower in direct proportion to the number of members.

How could that be improved? Letâ€™s suppose all these club members had a member `ID`, which happened to be a sequential number reflecting the order in which they joined the club.

Assuming that searching by `ID` were acceptable, we could place all members in an array, with their indexes matching their `IDs` (for example, a member with `ID=10` would be at the **index** `10` in the array). This would allow us to access each member directly, with no search at all. That would be very efficient, in fact, as efficient as it can possibly be, corresponding to the lowest complexity possible, `O(1)`, also known as **constant time**.

But, admittedly, our club member `ID` scenario is somewhat contrived(äººä¸ºçš„). What if `IDs` were big, non-sequential or random numbers? Or, if searching by ID were not acceptable, and we needed to search by name (or some other field) instead? It would certainly be useful to keep our fast direct access (or something close) while at the same time being able to handle arbitrary datasets and less restrictive search criteria.

**Hereâ€™s where hash functions come to the rescue. A suitable hash function can be used to map an arbitrary piece of data to an integer, which will play a similar role to that of our club member `ID`, albeit with a few important differences**.

First, a good hash function generally has a wide output range (typically, the whole range of a `32` or `64`-bit integer), so building an array for all possible indices would be either impractical or plain impossible, and a colossal(å·¨å¤§çš„) waste of memory. To overcome that, we can have a reasonably sized array (say, just twice the number of elements we expect to store) and perform a modulo operation on the hash to get the array index. So, the index would be `index = hash(object) mod N`, where `N` is the size of the array.

Second, object hashes will not be unique (unless weâ€™re working with a fixed dataset and a custom-built [perfect hash function](https://en.wikipedia.org/wiki/Perfect_hash_function), but we wonâ€™t discuss that here). There will be **collisions** (further increased by the modulo operation), and therefore a simple direct index access wonâ€™t work. There are several ways to handle this, but a typical one is to attach a list, commonly known as a **bucket**, to each array index to hold all the objects sharing a given index.

So, we have an array of size `N`, **with each entry pointing to an object bucket**. To add a new object, we need to calculate its `hash modulo N`, and check the bucket at the resulting index, adding the object if itâ€™s not already there. To search for an object, we do the same, just looking into the bucket to check if the object is there. **Such a structure is called a hash table**, and although the searches within buckets are linear, a properly sized hash table should have a reasonably small number of objects per bucket, resulting in almost constant time access.

With complex objects, the hash function is typically not applied to the whole object, but to a key instead. In our club member example, each object might contain several fields (like name, age, address, email, phone), but we could pick, say, the email to act as the key so that the hash function would be applied to the email only. In fact, the key need not be part of the object; it is common to store key/value pairs, where the key is usually a relatively short string, and the value can be an arbitrary piece of data. **In such cases, the hash table or hash map is used as a dictionary**, and thatâ€™s the way some high-level languages implement objects or associative arrays.


# Scaling Out: Distributed Hashing

Now that we have discussed hashing, weâ€™re ready to look into **distributed hashing**.

In some situations, it may be necessary or desirable to split a hash table into several parts, hosted by different servers. One of the main motivations for this is to bypass the memory limitations of using a single computer, allowing for the construction of arbitrarily large hash tables (given enough servers).

In such a scenario, the objects (and their keys) are distributed among several servers, hence the name.

A typical use case for this is the implementation of in-memory caches, such as [Memcached](https://en.wikipedia.org/wiki/Memcached).

Such setups consist of a pool of caching servers that host many key/value pairs and are used to provide fast access to data originally stored (or computed) elsewhere. For example, to reduce the load on a database server and at the same time improve performance, an application can be designed to first fetch data from the cache servers, and only if itâ€™s not present thereâ€”a situation known as **cache miss**â€”resort to the database, running the relevant query and caching the results with an appropriate key, so that it can be found next time itâ€™s needed.

**Now, how does distribution take place? What criteria(æ ‡å‡†) are used to determine which keys to host in which servers?**

The simplest way is to take the hash modulo of the number of servers. That is, `server = hash(key) mod N`, where `N` is the size of the pool. To store or retrieve a key, the client first computes the hash, applies a `modulo N` operation, and uses the resulting index to contact the appropriate server (probably by using a lookup table of IP addresses). **Note that the hash function used for key distribution must be the same one across all clients, but it need not be the same one used internally by the caching servers**.

# The Rehashing Problem

This distribution scheme is simple, intuitive, and works fine. That is, **until the number of servers changes**. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server, of course. The same applies if one or more new servers are added to the pool; keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo `N` will change, so most keys will need to be moved to a different server. **So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server**.

**So, most queries will result in misses, and the original data will likely need retrieving again from the source to be rehashed, thus placing a heavy load on the origin server(s) (typically a database). This may very well degrade performance severely and possibly crash the origin servers**.


# The Solution: Consistent Hashing

So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers, so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such schemeâ€”a clever, yet surprisingly simple oneâ€”is called **consistent hashing**, and was first described by [Karger et al. at MIT](http://courses.cse.tamu.edu/caverlee/csce438/readings/consistent-hashing.pdf) in an academic paper from 1997 (according to Wikipedia).

**Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle, or hash ring. This allows servers and objects to scale without affecting the overall system**.

Imagine we mapped the hash output range onto the edge of a circle. **That means that the minimum possible hash value, zero, would correspond to an angle of zero, the maximum possible value (some big integer weâ€™ll call `INT_MAX`) would correspond to an angle of `2ğ…` radians (or 360 degrees), and all other hash values would linearly fit somewhere in between**. So, we could take a key, compute its hash, and find out where it lies on the circleâ€™s edge. Assuming an `INT_MAX` of `1010` (for exampleâ€™s sake), the keys from our previous example would look like this:

![chash1](/assets/images/202405/chash1.png)

![chash2](/assets/images/202405/chash2.png)

Now imagine we also placed the **servers** on the edge of the circle, by pseudo-randomly assigning them angles too. This should be done in a repeatable way (or at least in such a way that all clients agree on the serversâ€™ angles). A convenient way of doing this is by hashing the server name (or IP address, or some ID)â€”as weâ€™d do with any other keyâ€”to come up with its angle.

In our example, things might look like this:

![chash3](/assets/images/202405/chash3.png)

![chash4](/assets/images/202405/chash4.png)

**Since we have the keys for both the `objects` and the `servers` on the same circle, we may define a simple rule to associate the former with the latter: Each object key will belong in the server whose key is closest, in a counterclockwise direction (or clockwise, depending on the conventions used)**. In other words, to find out which server to ask for a given key, we need to locate the key on the circle and move in the ascending angle direction until we find a server.

In our example:

![chash5](/assets/images/202405/chash5.png)

![chash6](/assets/images/202405/chash6.png)

From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list.

**To ensure object keys are evenly distributed among servers, we need to apply a simple trick (ä¸ºäº†è®© objects åœ¨ servers ä¸­åˆ†å¸ƒæ›´å‡åŒ€): To assign not one, but many labels (angles) to each server**. So instead of having labels `A`, `B` and `C`, we could have, say, `A0 .. A9, B0 .. B9` and `C0 .. C9`, all interspersed along the circle. The factor by which to increase the number of labels (server keys), known as **weight**, depends on the situation (**and may even be different for each server**) to adjust the probability of keys ending up on each. **For example, if server `B` were twice as powerful as the rest, it could be assigned twice as many labels, and as a result, it would end up holding twice as many objects (on average)**.

For our example weâ€™ll assume all three servers have an equal weight of `10` (this works well for three servers, for `10` to `50` servers, a weight in the range `100` to `500` would work better, and bigger pools may need even higher weights):


![chash7](/assets/images/202405/chash7.png)

![chash8](/assets/images/202405/chash8.png)

**So, whatâ€™s the benefit of all this circle approach**? Imagine server `C` is removed. To account for this, we must remove labels `C0 .. C9` from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled `Ax` and `Bx`, reassigning them to servers `A` and `B`.

**But what happens with the other object keys, the ones that originally belonged in A and B? Nothing! (éå¸¸å¥½çš„è®¾è®¡ï¼ŒC çš„åˆ é™¤ï¼Œä¸ä¼šå½±å“åŸæœ¬ A å’Œ B çš„æ•°æ®)** Thatâ€™s the beauty of it: The absence of `Cx` labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, **leaving all other keys untouched (å…¶ä»– Key ä¸ä¼šå—å½±å“)**:

![chash9](/assets/images/202405/chash9.png)

![chash10](/assets/images/202405/chash10.png)

Something similar happens if, instead of removing a server, **we add one (å¢åŠ  server çš„æƒ…å†µ)**. If we wanted to add server `D` to our example (say, as a replacement for `C`), we would need to add labels `D0 .. D9`. The result would be that roughly one-third of the existing keys (all belonging to `A` or `B`) would be reassigned to `D`, and, again, the rest would stay the same:

![chash11](/assets/images/202405/chash9.png)

![chash12](/assets/images/202405/chash10.png)


This is how consistent hashing solves the rehashing problem.

> In general, only `k/N` keys need to be `remapped` when `k` is the number of `keys` and `N` is the number of `servers` (more specifically, the maximum of the initial and final number of servers).


> We observed that when using distributed caching to optimize performance, it may happen that the number of caching servers changes (reasons for this may be a server crashing, or the need to add or remove a server to increase or decrease overall capacity). By using consistent hashing to distribute keys between the servers, we can rest assured that should that happen, the number of keys being rehashedâ€”and therefore, the impact on origin serversâ€”will be minimized, preventing potential downtime or performance issues.
>
> There are clients for several systems, such as `Memcached` and `Redis`, that include support for consistent hashing out of the box.
>
> Alternatively, you can implement the algorithm yourself, in your language of choice, and that should be relatively easy once the concept is understood.



# History

The term "consistent hashing" was introduced by [David Karger](https://en.wikipedia.org/wiki/David_Karger) et al. at MIT for use in [distributed caching](https://en.wikipedia.org/wiki/Distributed_cache), particularly for the web. This academic paper from 1997 in [Symposium on Theory of Computing](https://en.wikipedia.org/wiki/Symposium_on_Theory_of_Computing) introduced the term "consistent hashing" as a way of distributing requests among a changing population of web servers.

Each slot is then represented by a server in a distributed system or cluster. The addition of a server and the removal of a server (during scalability or outage) requires only `num_keys / num_slots` items to be re-shuffled when the number of slots (i.e. servers) change.

The authors mention [linear hashing](https://en.wikipedia.org/wiki/Linear_hashing) and its ability to handle sequential server addition and removal, while consistent hashing allows servers to be added and removed in an arbitrary order.

The paper was later re-purposed to address technical challenge of keeping track of a file in [peer-to-peer networks](https://en.wikipedia.org/wiki/Peer-to-peer) such as a [distributed hash table](https://en.wikipedia.org/wiki/Distributed_hash_table).

[Rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing), designed in 1996, is a simpler and more general technique. It achieves the goals of consistent hashing using the very different highest random weight (`HRW`) algorithm.


# Basic technique

In the problem of [load balancing](https://en.wikipedia.org/wiki/Load_balancing_(computing)), for example, when a [BLOB](https://en.wikipedia.org/wiki/Binary_large_object) has to be assigned to one of `ğ‘›` servers on a cluster, a standard hash function could be used in such a way that we calculate the hash value for that BLOB, assuming the resultant value of the hash is `ğ›½`, we perform [modular operation](https://en.wikipedia.org/wiki/Modular_arithmetic) with the number of servers (`ğ‘›` in this case) to determine the server in which we can place the BLOB: `ğœ = ğ›½ % ğ‘›`; hence the BLOB will be placed in the server whose server ID is successor of `ğœ` in this case. However, when a server is added or removed during outage or scaling (when `ğ‘›` changes), all the BLOBs in every server should be reassigned and moved due to rehashing, but this operation is expensive.

Consistent hashing was designed to avoid the problem of having to reassign every BLOB when a server is added or removed throughout the cluster. The central idea is to use a hash function that maps both the BLOB and servers to a unit circle, usually `2ğœ‹` radians.

For example, `ğœ = Î¦ % 360` (where `Î¦` is hash of a BLOB or server's identifier, like IP address or UUID). Each BLOB is then assigned to the next server that appears on the circle in clockwise order. Usually, [binary search algorithm](https://en.wikipedia.org/wiki/Binary_search_algorithm) or [linear search](https://en.wikipedia.org/wiki/Linear_search) is used to find a "spot" or server to place that particular BLOB in `ğ‘‚(logğ‘)` or `ğ‘‚(ğ‘)` complexities respectively; and in every iteration, which happens in clockwise manner, an operation `ğœ â‰¤ Î¨` (where `Î¨` is the value of the server within the cluster) is performed to find the server to place the BLOB.

**This provides an even distribution of BLOBs to servers. But, more importantly, if a server fails and is removed from the circle, only the BLOBs that were mapped to the failed server need to be reassigned to the next server in clockwise order. Likewise, if a new server is added, it is added to the unit circle, and only the BLOBs mapped to that server need to be reassigned**.

**Importantly, when a server is added or removed, the vast majority of the BLOBs maintain their prior server assignments, and the addition of `ğ‘›`th server only causes `1/ğ‘›` fraction of the BLOBs to relocate**.

Although the process of moving BLOBs across cache servers in the cluster depends on the context, commonly, the newly added cache server identifies its "successor" and moves all the BLOBs, whose mapping belongs to this server (i.e. whose hash value is less than that of the new server), from it. However, in the case of [web page caches](https://en.wikipedia.org/wiki/Web_cache), in most implementations there is no involvement of moving or copying, assuming the cached BLOB is small enough. When a request hits a newly added cache server, a [cache miss](https://en.wikipedia.org/wiki/Cache_(computing)#CACHE-MISS) happens and a request to the actual web server is made and the BLOB is cached locally for future requests. The redundant BLOBs on the previously used cache servers would be removed as per the cache eviction policies.

![chash_demo](/assets/images/202405/chash_demo.png)


# Comparison with rendezvous hashing and other alternatives

[Rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing), designed in 1996, is a simpler and more general technique, and permits fully distributed agreement on a set of `ğ‘˜` options out of a possible set of `ğ‘›` options. [It can in fact be shown](https://en.wikipedia.org/wiki/Rendezvous_hashing#Comparison_with_Consistent_Hashing) that consistent hashing is a special case of rendezvous hashing. Because of its simplicity and generality, rendezvous hashing is now being used in place of Consistent Hashing in many applications.

If key values will always increase [monotonically](https://en.wikipedia.org/wiki/Monotonic), an alternative approach using a [hash table with monotonic keys](https://en.wikipedia.org/wiki/Hash_table#Monotonic_keys) may be more suitable than consistent hashing.


# Complexity

![chash_demo2](/assets/images/202405/chash_demo2.png)


# gcc 4.8.5 std::hash å®ç°

å‚è€ƒ `libstdc++-v3/libsupc++/hash_bytes.h` å®ç°ã€‚

``` cpp
/** @file bits/hash_bytes.h
 *  This is an internal header file, included by other library headers.
 *  Do not attempt to use it directly. @headername{functional}
 */

#ifndef _HASH_BYTES_H
#define _HASH_BYTES_H 1

#pragma GCC system_header

#include <bits/c++config.h>

namespace std
{
_GLIBCXX_BEGIN_NAMESPACE_VERSION

  // Hash function implementation for the nontrivial specialization.
  // All of them are based on a primitive that hashes a pointer to a
  // byte array. The actual hash algorithm is not guaranteed to stay
  // the same from release to release -- it may be updated or tuned to
  // improve hash quality or speed.
  size_t
  _Hash_bytes(const void* __ptr, size_t __len, size_t __seed);

  // A similar hash primitive, using the FNV hash algorithm. This
  // algorithm is guaranteed to stay the same from release to release.
  // (although it might not produce the same values on different
  // machines.)
  size_t
  _Fnv_hash_bytes(const void* __ptr, size_t __len, size_t __seed);

_GLIBCXX_END_NAMESPACE_VERSION
} // namespace
```

`_Hash_bytes` ä½¿ç”¨ Murmur hash for 64-bit size_t çš„å®ç°ï¼Œ`_Fnv_hash_bytes` ä½¿ç”¨ FNV hash for 64-bit size_t çš„å®ç°ã€‚

``` cpp
#elif __SIZEOF_SIZE_T__ == 8

  // Implementation of Murmur hash for 64-bit size_t.
  size_t
  _Hash_bytes(const void* ptr, size_t len, size_t seed)
  {
    static const size_t mul = (((size_t) 0xc6a4a793UL) << 32UL)
			      + (size_t) 0x5bd1e995UL;
    const char* const buf = static_cast<const char*>(ptr);

    // Remove the bytes not divisible by the sizeof(size_t).  This
    // allows the main loop to process the data as 64-bit integers.
    const int len_aligned = len & ~0x7;
    const char* const end = buf + len_aligned;
    size_t hash = seed ^ (len * mul);
    for (const char* p = buf; p != end; p += 8)
      {
	const size_t data = shift_mix(unaligned_load(p) * mul) * mul;
	hash ^= data;
	hash *= mul;
      }
    if ((len & 0x7) != 0)
      {
	const size_t data = load_bytes(end, len & 0x7);
	hash ^= data;
	hash *= mul;
      }
    hash = shift_mix(hash) * mul;
    hash = shift_mix(hash);
    return hash;
  }

  // Implementation of FNV hash for 64-bit size_t.
  size_t
  _Fnv_hash_bytes(const void* ptr, size_t len, size_t hash)
  {
    const char* cptr = static_cast<const char*>(ptr);
    for (; len; --len)
      {
	hash ^= static_cast<size_t>(*cptr++);
	hash *= static_cast<size_t>(1099511628211ULL);
      }
    return hash;
  }
```

# std::hash

``` cpp
template< class Key >  // (since C++11)
struct hash;
```

The unordered associative containers `std::unordered_set`, `std::unordered_multiset`, `std::unordered_map`, `std::unordered_multimap` use specializations of the template `std::hash` as the default hash function.


# ä¸€è‡´æ€§å“ˆå¸Œå®ç°

## å‰²ç¯æ³•

å®ç°å‚è€ƒï¼šhttps://github.com/RJ/ketama (C library for consistent hashing, and langauge bindings)


## [Rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing)

è¿™ä¸ªç®—æ³•æ¯”è¾ƒç®€å•ç²—æš´ï¼Œæ²¡æœ‰ä»€ä¹ˆæ„é€ ç¯æˆ–è€…å¤æ‚çš„è®¡ç®—è¿‡ç¨‹ï¼Œå®ƒå¯¹äºä¸€ä¸ªç»™å®šçš„ Keyï¼Œå¯¹æ¯ä¸ªèŠ‚ç‚¹éƒ½é€šè¿‡å“ˆå¸Œå‡½æ•° `h()` è®¡ç®—ä¸€ä¸ªæƒé‡å€¼ `wi,j = h(Keyi, Nodej)`ï¼Œç„¶ååœ¨æ‰€æœ‰çš„æƒé‡å€¼ä¸­é€‰æ‹©æœ€å¤§ä¸€ä¸ª `Max{wi,j}`ã€‚æ˜¾è€Œæ˜“è§ï¼Œç®—æ³•æŒºç®€å•ï¼Œæ‰€éœ€å­˜å‚¨ç©ºé—´ä¹Ÿå¾ˆå°ï¼Œä½†ç®—æ³•çš„å¤æ‚åº¦æ˜¯ `O(n)`ã€‚


## Jump consistent hash

Jump consistent hash æ˜¯ Google äº 2014 å¹´å‘è¡¨çš„[è®ºæ–‡](https://arxiv.org/ftp/arxiv/papers/1406/1406.2294.pdf)ä¸­æå‡ºçš„ä¸€ç§ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ï¼Œå®ƒå ç”¨å†…å­˜å°ä¸”é€Ÿåº¦å¾ˆå¿«ï¼Œå¹¶ä¸”åªæœ‰å¤§æ¦‚ 5 è¡Œä»£ç ï¼Œæ¯”è¾ƒé€‚åˆç”¨åœ¨åˆ† shard çš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿä¸­ã€‚å…¶å®Œæ•´çš„ä»£ç å¦‚ä¸‹ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ª 64 ä½çš„ Key åŠæ¡¶çš„æ•°é‡ï¼Œè¾“å‡ºæ˜¯è¿”å›è¿™ä¸ª Key è¢«åˆ†é…åˆ°çš„æ¡¶çš„ç¼–å·ã€‚

``` cpp
int32_t JumpConsistentHash(uint64_t key, int32_t num_buckets) {
    int64_t b = Â­-1, j = 0Í¾
    while (j < num_buckets) {
        b = jÍ¾
        key = key * 2862933555777941757ULL + 1Í¾
        j = (b + 1) * (double(1LL << 31) / double((key >> 33) + 1))Í¾
    }
    return bÍ¾
}
```

> Jump consistent hash æœ‰ä¸€ä¸ªæ¯”è¾ƒæ˜æ˜¾çš„ç¼ºç‚¹ï¼Œå®ƒåªèƒ½åœ¨å°¾éƒ¨å¢åˆ èŠ‚ç‚¹ï¼Œè€Œä¸å¤ªå¥½åœ¨ä¸­é—´å¢åˆ ã€‚

## Maglev hash

Maglev hash æ˜¯ Google äº 2016 å¹´å‘è¡¨çš„ä¸€ç¯‡[è®ºæ–‡](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/44824.pdf)ä¸­æå‡ºæ¥çš„ä¸€ç§æ–°çš„ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ã€‚Maglev hash çš„åŸºæœ¬æ€è·¯æ˜¯å»ºç«‹ä¸€å¼ ä¸€ç»´çš„æŸ¥æ‰¾è¡¨ï¼Œä¸€ä¸ªé•¿åº¦ä¸º `M` çš„åˆ—è¡¨ï¼Œè®°å½•ç€æ¯ä¸ªä½ç½®æ‰€å±çš„èŠ‚ç‚¹ç¼–å· `B0...BN`ï¼Œå½“éœ€è¦åˆ¤æ–­æŸä¸ª Key è¢«åˆ†é…åˆ°å“ªä¸ªèŠ‚ç‚¹çš„æ—¶å€™ï¼Œåªéœ€å¯¹ Key è®¡ç®— hashï¼Œç„¶åå¯¹ `M` å–æ¨¡çœ‹æ‰€è½åˆ°çš„ä½ç½®å±äºå“ªä¸ªèŠ‚ç‚¹ã€‚

> å¦‚ä½•æŸ¥æ‰¾çœ‹ä¸Šå»å¾ˆç®€å•ï¼Œé—®é¢˜æ˜¯å¦‚ä½•äº§ç”Ÿè¿™ä¸ªæŸ¥æ‰¾è¡¨ã€‚

## [MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)

`MurmurHash` is a non-cryptographic hash function suitable for general hash-based lookup. It was created by Austin Appleby in 2008 and is currently hosted on GitHub along with its test suite named 'SMHasher'. It also exists in a number of variants, all of which have been released into the public domain. The name comes from two basic operations, **multiply** (`MU`) and **rotate** (`R`), used in its inner loop.

Unlike cryptographic hash functions, it is not specifically designed to be difficult to reverse by an adversary, making it unsuitable for cryptographic purposes.

> MurmurHash æ˜¯ä¸€ç§éåŠ å¯†çš„å“ˆå¸Œå‡½æ•°ï¼Œé€‚ç”¨äºåŸºäºå“ˆå¸Œçš„ä¸€èˆ¬æŸ¥æ‰¾ã€‚å®ƒæ˜¯ç”± Austin Appleby åœ¨ 2008 å¹´åˆ›å»ºçš„ï¼Œå¹¶ä¸”ç›®å‰åœ¨ GitHub ä¸Šæ‰˜ç®¡ï¼Œè¿åŒå…¶åä¸º 'SMHasher' çš„æµ‹è¯•å¥—ä»¶ã€‚å®ƒä¹Ÿå­˜åœ¨äºå¤šç§å˜ä½“ä¸­ï¼Œæ‰€æœ‰è¿™äº›å˜ä½“éƒ½å·²ç»å‘å¸ƒåˆ°å…¬å…±é¢†åŸŸã€‚å®ƒçš„åå­—æ¥æºäºå…¶å†…éƒ¨å¾ªç¯ä¸­ä½¿ç”¨çš„ä¸¤ä¸ªåŸºæœ¬æ“ä½œï¼Œä¹˜æ³•ï¼ˆMUï¼‰å’Œæ—‹è½¬ï¼ˆRï¼‰ã€‚
>
> ä¸åŠ å¯†å“ˆå¸Œå‡½æ•°ä¸åŒï¼ŒMurmurHash å¹¶æœªç‰¹åˆ«è®¾è®¡æˆå¯¹æŠ—æ‰‹æ–¹éš¾ä»¥é€†è½¬ï¼Œè¿™ä½¿å¾—å®ƒä¸é€‚åˆç”¨äºåŠ å¯†ç›®çš„ã€‚


MurmurHash æœ¬èº«ä¸æ˜¯ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ªé€šç”¨çš„éåŠ å¯†å“ˆå¸Œå‡½æ•°ã€‚ç„¶è€Œï¼Œå®ƒå¯ä»¥ç”¨ä½œä¸€è‡´æ€§å“ˆå¸Œç®—æ³•çš„åº•å±‚å“ˆå¸Œå‡½æ•°ã€‚è®©æˆ‘ä»¬æ¥åŒºåˆ†è¿™ä¸¤ä¸ªæ¦‚å¿µï¼š

* **MurmurHash** æ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼Œå®ƒå°†ä»»æ„é•¿åº¦çš„è¾“å…¥æ•°æ®ï¼ˆä¾‹å¦‚å­—ç¬¦ä¸²æˆ–äºŒè¿›åˆ¶æ•°æ®ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„å“ˆå¸Œå€¼ã€‚MurmurHash çš„è®¾è®¡ç›®æ ‡æ˜¯å¿«é€Ÿä¸”åœ¨ä¸åŒè¾“å…¥æ•°æ®ä¸Šäº§ç”Ÿå‡åŒ€åˆ†å¸ƒçš„å“ˆå¸Œå€¼ï¼Œä»¥å‡å°‘å“ˆå¸Œå†²çªã€‚
* **ä¸€è‡´æ€§å“ˆå¸Œ** æ˜¯ä¸€ç§å“ˆå¸ŒæŠ€æœ¯ï¼Œé€šå¸¸ç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œä»¥å®ç°è´Ÿè½½å‡è¡¡å’Œæ•°æ®åˆ†ç‰‡ã€‚ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ä½¿ç”¨ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼ˆå¦‚ MurmurHashï¼‰å°†æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªç¯å½¢ç©ºé—´ï¼Œç„¶åå°†ç¯å½¢ç©ºé—´åˆ’åˆ†ç»™å¤šä¸ªèŠ‚ç‚¹ã€‚å½“èŠ‚ç‚¹åŠ å…¥æˆ–ç¦»å¼€ç³»ç»Ÿæ—¶ï¼Œä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ç¡®ä¿åªéœ€è¦é‡æ–°åˆ†é…å¾ˆå°‘çš„æ•°æ®ç‰‡æ®µï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†é…æ‰€æœ‰æ•°æ®ã€‚

**å› æ­¤ï¼ŒMurmurHash å¯ä»¥ä½œä¸ºä¸€è‡´æ€§å“ˆå¸Œç®—æ³•çš„åº•å±‚å“ˆå¸Œå‡½æ•°ï¼Œä½†å®ƒæœ¬èº«å¹¶ä¸æ˜¯ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ã€‚è¦å®ç°ä¸€è‡´æ€§å“ˆå¸Œï¼Œéœ€è¦åœ¨ MurmurHash çš„åŸºç¡€ä¸Šæ„å»ºä¸€ä¸ªåˆ†å¸ƒå¼å“ˆå¸Œç¯å’Œç›¸åº”çš„æ•°æ®åˆ†é…ç­–ç•¥**ã€‚

> MurmurHash3
>
> The current version is MurmurHash3, which yields a 32-bit or 128-bit hash value. When using 128-bits, the x86 and x64 versions do not produce the same values, as the algorithms are optimized for their respective platforms. MurmurHash3 was released alongside SMHasherâ€”a hash function test suite.





## å¯¹æ¯”

å¯¹ä»¥ä¸Šå››ç§ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•è¿›è¡Œå¯¹æ¯”æ€»ç»“ï¼š

| ç®—æ³• | æ‰©å®¹ | ç¼©å®¹ | å¹³è¡¡æ€§ | å•è°ƒæ€§ | ç¨³å®šæ€§ | æ—¶é—´å¤æ‚åº¦
| -- | -- | -- | -- | -- | -- | -- | -- | --
| Ketama | å¥½ | å¥½ | è¾ƒå¥½ | å¥½ | è¾ƒå¥½ | O(log vn)
| Rendezvous | å¥½ | å¥½ | è¾ƒå¥½ | å¥½ | è¾ƒå¥½ | O(n)
| Jump consistent hash | å¥½ | éœ€è¦é¢å¤–å¤„ç† | å¥½ | å¥½ | å¥½ | O(ln n)
| Maglev hash| è¾ƒå¥½ | è¾ƒå¥½ | å¥½ | è¾ƒå¥½ | è¾ƒå¥½ | O(MlogM)ï¼Œæœ€å O(M*M)



# Refer

* https://en.wikipedia.org/wiki/Consistent_hashing
* [Rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing)
* [A Guide to Consistent Hashing](https://www.toptal.com/big-data/consistent-hashing) (å¥½æ–‡)
* https://en.wikipedia.org/wiki/MurmurHash
* https://en.cppreference.com/w/cpp/utility/hash
* https://github.com/gcc-mirror/gcc/tags
  +  https://github.com/gcc-mirror/gcc/blob/releases/gcc-12.1.0/libstdc++-v3/include/bits/functional_hash.h
  +  https://github.com/gcc-mirror/gcc/blob/releases/gcc-12.1.0/libstdc++-v3/libsupc++/hash_bytes.cc